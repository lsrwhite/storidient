{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "transformer model 구현",
      "provenance": [],
      "authorship_tag": "ABX9TyPsnBnU6OKnMj6OZr7orKbL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lsrwhite/storidient/blob/main/transformer_model_%EA%B5%AC%ED%98%84.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgopm-RWi9Xy"
      },
      "source": [
        "# Seq2seq 모델 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7WCR9vSi3u8"
      },
      "source": [
        " \n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pack_padded_sequence as pack\n",
        "from torch.nn.utils.rnn import pad_packed_sequence as unpack\n",
        "\n",
        "import simple_nmt.data_loader as data_loader \n",
        "from simple_nmt.search import SingleBeamSearchBoard\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden_size):\n",
        "        super(Attention, self).__init__()\n",
        "\n",
        "        self.linear = nn.Linear(hidden_size, hidden_size, bias=False) #linear 변환\n",
        "        self.softmax = nn.Softmax(dim=-1)  #어텐션 웨이트\n",
        "\n",
        "    def forward(self, h_src, h_t_tgt, mask=None):\n",
        "        # |h_src| = (batch_size, length, hidden_size)\n",
        "        # |h_t_tgt| = (batch_size, 1, hidden_size)  #one timestep \n",
        "        # |mask| = (batch_size, length) #우리가 원하는 위치에 -무한대 #pad에 true가 있을것.\n",
        "\n",
        "        query = self.linear(h_t_tgt)\n",
        "        # |query| = (batch_size, 1, hidden_size) (배치, 한 타임스텝, 히든사이즈)\n",
        "\n",
        "        weight = torch.bmm(query, h_src.transpose(1, 2))\n",
        "        # |weight| = (batch_size, 1, length) #softmax 전 W의 크기\n",
        "        if mask is not None:\n",
        "            # Set each weight as -inf, if the mask value equals to 1.\n",
        "            # Since the softmax operation makes -inf to 0,\n",
        "            # masked weights would be set to 0 after softmax operation.\n",
        "            # Thus, if the sample is shorter than other samples in mini-batch,\n",
        "            # the weight for empty time-step would be set to 0.\n",
        "            weight.masked_fill_(mask.unsqueeze(1), -float('inf'))\n",
        "        weight = self.softmax(weight)\n",
        "\n",
        "        context_vector = torch.bmm(weight, h_src)\n",
        "        # |context_vector| = (batch_size, 1, hidden_size)\n",
        "\n",
        "        return context_vector  #출력은 컨텍스트 벡터 \n",
        "\n",
        "print("Hello")",
        "\n",
        "class Encoder(nn.Module):\n",
        "\n",
        "    def __init__(self, word_vec_size, hidden_size, n_layers=4, dropout_p=.2):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        # Be aware of value of 'batch_first' parameter.\n",
        "        # Also, its hidden_size is half of original hidden_size,\n",
        "        # because it is bidirectional.\n",
        "        self.rnn = nn.LSTM(\n",
        "            word_vec_size,\n",
        "            int(hidden_size / 2), #정방향과 역방향 모두를 구현 \n",
        "            num_layers=n_layers,\n",
        "            dropout=dropout_p,\n",
        "            bidirectional=True, #양방향\n",
        "            batch_first=True, #첫번째부터 배치가 들어가게. 까먹으면 안 됨\n",
        "        )\n",
        "\n",
        "    def forward(self, emb):\n",
        "        # |emb| = (batch_size, length, word_vec_size)\n",
        "\n",
        "        if isinstance(emb, tuple): #자료형 확인\n",
        "            x, lengths = emb\n",
        "            x = pack(x, lengths.tolist(), batch_first=True)\n",
        "\n",
        "            # Below is how pack_padded_sequence works.\n",
        "            # As you can see,\n",
        "            # PackedSequence object has information about mini-batch-wise information,\n",
        "            # not time-step-wise information.\n",
        "            # \n",
        "            # a = [torch.tensor([1,2,3]), torch.tensor([3,4])]\n",
        "            # b = torch.nn.utils.rnn.pad_sequence(a, batch_first=True)  #패드시퀀스\n",
        "            # >>>>\n",
        "            # tensor([[ 1,  2,  3],\n",
        "            #     [ 3,  4,  0]])  #임베딩 텐서의 모양\n",
        "            # torch.nn.utils.rnn.pack_padded_sequence(b, batch_first=True, lengths=[3,2]\n",
        "            # >>>>PackedSequence(data=tensor([ 1,  3,  2,  4,  3]), batch_sizes=tensor([ 2,  2,  1])) #마지막 타임 스텝에서는 패드가 없어서 2, 2, 1\n",
        "        else:\n",
        "            x = emb\n",
        "\n",
        "        y, h = self.rnn(x)\n",
        "        # |y| = (batch_size, length, hidden_size)\n",
        "        # |h[0]| = (num_layers * 2, batch_size, hidden_size / 2) #마지막 타임스텝의 히든\n",
        "\n",
        "        if isinstance(emb, tuple):\n",
        "            y, _ = unpack(y, batch_first=True) #언팩을 해줌\n",
        "\n",
        "        return y, h\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "\n",
        "    def __init__(self, word_vec_size, hidden_size, n_layers=4, dropout_p=.2):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        # Be aware of value of 'batch_first' parameter and 'bidirectional' parameter.\n",
        "        self.rnn = nn.LSTM(\n",
        "            word_vec_size + hidden_size, #ws + hs #input feeding\n",
        "            hidden_size,\n",
        "            num_layers=n_layers,\n",
        "            dropout=dropout_p,\n",
        "            bidirectional=False,\n",
        "            batch_first=True, #꼭 True로\n",
        "        )\n",
        "\n",
        "    def forward(self, emb_t, h_t_1_tilde, h_t_1): #인코더와 달리 디코더에서는 한 타임스텝씩 들어옴 #이전타임의 hs도 받아야 \n",
        "        # |emb_t| = (batch_size, 1, word_vec_size)\n",
        "        # |h_t_1_tilde| = (batch_size, 1, hidden_size)\n",
        "        # |h_t_1[0]| = (n_layers, batch_size, hidden_size)\n",
        "        batch_size = emb_t.size(0) #bs\n",
        "        hidden_size = h_t_1[0].size(-1)\n",
        "\n",
        "        if h_t_1_tilde is None: #first timestep\n",
        "            # If this is the first time-step,\n",
        "            h_t_1_tilde = emb_t.new(batch_size, 1, hidden_size).zero_()\n",
        "\n",
        "        # Input feeding trick.\n",
        "        x = torch.cat([emb_t, h_t_1_tilde], dim=-1) # 마지막 인덱스에 대해 cat #(bs, 1, hs)\n",
        "        #h = (n_layer, bs, hs)\n",
        "\n",
        "        # Unlike encoder, decoder must take an input for sequentially.\n",
        "        y, h = self.rnn(x, h_t_1)\n",
        "\n",
        "        return y, h # 매 타임스텝마다 출력과 h을 반환\n",
        "\n",
        "\n",
        "class Generator(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        self.output = nn.Linear(hidden_size, output_size) #선형변환\n",
        "        self.softmax = nn.LogSoftmax(dim=-1) #로그softmax\n",
        "\n",
        "    def forward(self, x):\n",
        "        # |x| = (batch_size, length, hidden_size)\n",
        "\n",
        "        y = self.softmax(self.output(x))\n",
        "        # |y| = (batch_size, length, output_size) #미니배치 내의, 샘플당, 타임스텝당 각 단어별 확률분포\n",
        "\n",
        "        # Return log-probability instead of just probability.\n",
        "        return y\n",
        "\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size,\n",
        "        word_vec_size,\n",
        "        hidden_size,\n",
        "        output_size,\n",
        "        n_layers=4,\n",
        "        dropout_p=.2\n",
        "    ):\n",
        "        self.input_size = input_size\n",
        "        self.word_vec_size = word_vec_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.dropout_p = dropout_p\n",
        "\n",
        "        super(Seq2Seq, self).__init__()\n",
        "\n",
        "        self.emb_src = nn.Embedding(input_size, word_vec_size)\n",
        "        self.emb_dec = nn.Embedding(output_size, word_vec_size)\n",
        "\n",
        "        self.encoder = Encoder(\n",
        "            word_vec_size, hidden_size,\n",
        "            n_layers=n_layers, dropout_p=dropout_p,\n",
        "        )\n",
        "        self.decoder = Decoder(\n",
        "            word_vec_size, hidden_size,\n",
        "            n_layers=n_layers, dropout_p=dropout_p,\n",
        "        )\n",
        "        self.attn = Attention(hidden_size)\n",
        "\n",
        "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.generator = Generator(hidden_size, output_size)\n",
        "\n",
        "    def generate_mask(self, x, length):\n",
        "        mask = []\n",
        "\n",
        "        max_length = max(length)\n",
        "        for l in length:\n",
        "            if max_length - l > 0:\n",
        "                # If the length is shorter than maximum length among samples, \n",
        "                # set last few values to be 1s to remove attention weight.\n",
        "                mask += [torch.cat([x.new_ones(1, l).zero_(),\n",
        "                                    x.new_ones(1, (max_length - l))\n",
        "                                    ], dim=-1)]\n",
        "            else:\n",
        "                # If the length of the sample equals to maximum length among samples, \n",
        "                # set every value in mask to be 0.\n",
        "                mask += [x.new_ones(1, l).zero_()]\n",
        "\n",
        "        mask = torch.cat(mask, dim=0).bool()\n",
        "\n",
        "        return mask\n",
        "\n",
        "    def merge_encoder_hiddens(self, encoder_hiddens):\n",
        "        new_hiddens = []\n",
        "        new_cells = []\n",
        "\n",
        "        hiddens, cells = encoder_hiddens\n",
        "\n",
        "        # i-th and (i+1)-th layer is opposite direction.\n",
        "        # Also, each direction of layer is half hidden size.\n",
        "        # Therefore, we concatenate both directions to 1 hidden size layer.\n",
        "        for i in range(0, hiddens.size(0), 2):\n",
        "            new_hiddens += [torch.cat([hiddens[i], hiddens[i + 1]], dim=-1)]\n",
        "            new_cells += [torch.cat([cells[i], cells[i + 1]], dim=-1)]\n",
        "\n",
        "        new_hiddens, new_cells = torch.stack(new_hiddens), torch.stack(new_cells)\n",
        "\n",
        "        return (new_hiddens, new_cells)\n",
        "\n",
        "    def fast_merge_encoder_hiddens(self, encoder_hiddens):\n",
        "        # Merge bidirectional to uni-directional\n",
        "        # We need to convert size from (n_layers * 2, batch_size, hidden_size / 2)\n",
        "        # to (n_layers, batch_size, hidden_size).\n",
        "        # Thus, the converting operation will not working with just 'view' method.\n",
        "        h_0_tgt, c_0_tgt = encoder_hiddens\n",
        "        batch_size = h_0_tgt.size(1)\n",
        "\n",
        "        h_0_tgt = h_0_tgt.transpose(0, 1).contiguous().view(batch_size,\n",
        "                                                            -1,\n",
        "                                                            self.hidden_size\n",
        "                                                            ).transpose(0, 1).contiguous()\n",
        "        c_0_tgt = c_0_tgt.transpose(0, 1).contiguous().view(batch_size,\n",
        "                                                            -1,\n",
        "                                                            self.hidden_size\n",
        "                                                            ).transpose(0, 1).contiguous()\n",
        "        # You can use 'merge_encoder_hiddens' method, instead of using above 3 lines.\n",
        "        # 'merge_encoder_hiddens' method works with non-parallel way.\n",
        "        # h_0_tgt = self.merge_encoder_hiddens(h_0_tgt)\n",
        "\n",
        "        # |h_src| = (batch_size, length, hidden_size)\n",
        "        # |h_0_tgt| = (n_layers, batch_size, hidden_size)\n",
        "        return h_0_tgt, c_0_tgt\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        batch_size = tgt.size(0)\n",
        "\n",
        "        mask = None\n",
        "        x_length = None\n",
        "        if isinstance(src, tuple):\n",
        "            x, x_length = src\n",
        "            # Based on the length information, gererate mask to prevent that\n",
        "            # shorter sample has wasted attention.\n",
        "            mask = self.generate_mask(x, x_length)\n",
        "            # |mask| = (batch_size, length)\n",
        "        else:\n",
        "            x = src\n",
        "\n",
        "        if isinstance(tgt, tuple):\n",
        "            tgt = tgt[0]\n",
        "\n",
        "        # Get word embedding vectors for every time-step of input sentence.\n",
        "        emb_src = self.emb_src(x)\n",
        "        # |emb_src| = (batch_size, length, word_vec_size)\n",
        "\n",
        "        # The last hidden state of the encoder would be a initial hidden state of decoder.\n",
        "        h_src, h_0_tgt = self.encoder((emb_src, x_length))\n",
        "        # |h_src| = (batch_size, length, hidden_size)\n",
        "        # |h_0_tgt| = (n_layers * 2, batch_size, hidden_size / 2)\n",
        "\n",
        "        h_0_tgt = self.fast_merge_encoder_hiddens(h_0_tgt)\n",
        "        emb_tgt = self.emb_dec(tgt)\n",
        "        # |emb_tgt| = (batch_size, length, word_vec_size)\n",
        "        h_tilde = []\n",
        "\n",
        "        h_t_tilde = None\n",
        "        decoder_hidden = h_0_tgt\n",
        "        # Run decoder until the end of the time-step.\n",
        "        for t in range(tgt.size(1)):\n",
        "            # Teacher Forcing: take each input from training set,\n",
        "            # not from the last time-step's output.\n",
        "            # Because of Teacher Forcing,\n",
        "            # training procedure and inference procedure becomes different.\n",
        "            # Of course, because of sequential running in decoder,\n",
        "            # this causes severe bottle-neck.\n",
        "            emb_t = emb_tgt[:, t, :].unsqueeze(1)\n",
        "            # |emb_t| = (batch_size, 1, word_vec_size)\n",
        "            # |h_t_tilde| = (batch_size, 1, hidden_size)\n",
        "\n",
        "            decoder_output, decoder_hidden = self.decoder(emb_t,\n",
        "                                                          h_t_tilde,\n",
        "                                                          decoder_hidden\n",
        "                                                          )\n",
        "            # |decoder_output| = (batch_size, 1, hidden_size)\n",
        "            # |decoder_hidden| = (n_layers, batch_size, hidden_size)\n",
        "\n",
        "            context_vector = self.attn(h_src, decoder_output, mask)\n",
        "            # |context_vector| = (batch_size, 1, hidden_size)\n",
        "\n",
        "            h_t_tilde = self.tanh(self.concat(torch.cat([decoder_output,\n",
        "                                                         context_vector\n",
        "                                                         ], dim=-1)))\n",
        "            # |h_t_tilde| = (batch_size, 1, hidden_size)\n",
        "\n",
        "            h_tilde += [h_t_tilde]\n",
        "\n",
        "        h_tilde = torch.cat(h_tilde, dim=1)\n",
        "        # |h_tilde| = (batch_size, length, hidden_size)\n",
        "\n",
        "        y_hat = self.generator(h_tilde)\n",
        "        # |y_hat| = (batch_size, length, output_size)\n",
        "\n",
        "        return y_hat\n",
        "\n",
        "    def search(self, src, is_greedy=True, max_length=255):\n",
        "        if isinstance(src, tuple):\n",
        "            x, x_length = src\n",
        "            mask = self.generate_mask(x, x_length)\n",
        "        else:\n",
        "            x, x_length = src, None\n",
        "            mask = None\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # Same procedure as teacher forcing.\n",
        "        emb_src = self.emb_src(x)\n",
        "        h_src, h_0_tgt = self.encoder((emb_src, x_length))\n",
        "        decoder_hidden = self.fast_merge_encoder_hiddens(h_0_tgt)\n",
        "\n",
        "        # Fill a vector, which has 'batch_size' dimension, with BOS value.\n",
        "        y = x.new(batch_size, 1).zero_() + data_loader.BOS\n",
        "\n",
        "        is_decoding = x.new_ones(batch_size, 1).bool()\n",
        "        h_t_tilde, y_hats, indice = None, [], []\n",
        "        \n",
        "        # Repeat a loop while sum of 'is_decoding' flag is bigger than 0,\n",
        "        # or current time-step is smaller than maximum length.\n",
        "        while is_decoding.sum() > 0 and len(indice) < max_length:\n",
        "            # Unlike training procedure,\n",
        "            # take the last time-step's output during the inference.\n",
        "            emb_t = self.emb_dec(y)\n",
        "            # |emb_t| = (batch_size, 1, word_vec_size)\n",
        "\n",
        "            decoder_output, decoder_hidden = self.decoder(emb_t,\n",
        "                                                          h_t_tilde,\n",
        "                                                          decoder_hidden)\n",
        "            context_vector = self.attn(h_src, decoder_output, mask)\n",
        "            h_t_tilde = self.tanh(self.concat(torch.cat([decoder_output,\n",
        "                                                         context_vector\n",
        "                                                         ], dim=-1)))\n",
        "            y_hat = self.generator(h_t_tilde)\n",
        "            # |y_hat| = (batch_size, 1, output_size)\n",
        "            y_hats += [y_hat]\n",
        "\n",
        "            if is_greedy:\n",
        "                y = y_hat.argmax(dim=-1)\n",
        "                # |y| = (batch_size, 1)\n",
        "            else:\n",
        "                # Take a random sampling based on the multinoulli distribution.\n",
        "                y = torch.multinomial(y_hat.exp().view(batch_size, -1), 1)\n",
        "                # |y| = (batch_size, 1)\n",
        "\n",
        "            # Put PAD if the sample is done.\n",
        "            y = y.masked_fill_(~is_decoding, data_loader.PAD)\n",
        "            # Update is_decoding if there is EOS token.\n",
        "            is_decoding = is_decoding * torch.ne(y, data_loader.EOS)\n",
        "            # |is_decoding| = (batch_size, 1)\n",
        "            indice += [y]\n",
        "\n",
        "        y_hats = torch.cat(y_hats, dim=1)\n",
        "        indice = torch.cat(indice, dim=1)\n",
        "        # |y_hat| = (batch_size, length, output_size)\n",
        "        # |indice| = (batch_size, length)\n",
        "\n",
        "        return y_hats, indice\n",
        "\n",
        "    #profile\n",
        "    def batch_beam_search(\n",
        "        self,\n",
        "        src,\n",
        "        beam_size=5,\n",
        "        max_length=255,\n",
        "        n_best=1,\n",
        "        length_penalty=.2\n",
        "    ):\n",
        "        mask, x_length = None, None\n",
        "\n",
        "        if isinstance(src, tuple):\n",
        "            x, x_length = src\n",
        "            mask = self.generate_mask(x, x_length)\n",
        "            # |mask| = (batch_size, length)\n",
        "        else:\n",
        "            x = src\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        emb_src = self.emb_src(x)\n",
        "        h_src, h_0_tgt = self.encoder((emb_src, x_length))\n",
        "        # |h_src| = (batch_size, length, hidden_size)\n",
        "        h_0_tgt = self.fast_merge_encoder_hiddens(h_0_tgt)\n",
        "\n",
        "        # initialize 'SingleBeamSearchBoard' as many as batch_size\n",
        "        boards = [SingleBeamSearchBoard(\n",
        "            h_src.device,\n",
        "            {\n",
        "                'hidden_state': {\n",
        "                    'init_status': h_0_tgt[0][:, i, :].unsqueeze(1),\n",
        "                    'batch_dim_index': 1,\n",
        "                }, # |hidden_state| = (n_layers, batch_size, hidden_size)\n",
        "                'cell_state': {\n",
        "                    'init_status': h_0_tgt[1][:, i, :].unsqueeze(1),\n",
        "                    'batch_dim_index': 1,\n",
        "                }, # |cell_state| = (n_layers, batch_size, hidden_size)\n",
        "                'h_t_1_tilde': {\n",
        "                    'init_status': None,\n",
        "                    'batch_dim_index': 0,\n",
        "                }, # |h_t_1_tilde| = (batch_size, 1, hidden_size)\n",
        "            },\n",
        "            beam_size=beam_size,\n",
        "            max_length=max_length,\n",
        "        ) for i in range(batch_size)]\n",
        "        is_done = [board.is_done() for board in boards]\n",
        "\n",
        "        length = 0\n",
        "        # Run loop while sum of 'is_done' is smaller than batch_size, \n",
        "        # or length is still smaller than max_length.\n",
        "        while sum(is_done) < batch_size and length <= max_length:\n",
        "            # current_batch_size = sum(is_done) * beam_size\n",
        "\n",
        "            # Initialize fabricated variables.\n",
        "            # As far as batch-beam-search is running, \n",
        "            # temporary batch-size for fabricated mini-batch is \n",
        "            # 'beam_size'-times bigger than original batch_size.\n",
        "            fab_input, fab_hidden, fab_cell, fab_h_t_tilde = [], [], [], []\n",
        "            fab_h_src, fab_mask = [], []\n",
        "            \n",
        "            # Build fabricated mini-batch in non-parallel way.\n",
        "            # This may cause a bottle-neck.\n",
        "            for i, board in enumerate(boards):\n",
        "                # Batchify if the inference for the sample is still not finished.\n",
        "                if board.is_done() == 0:\n",
        "                    y_hat_i, prev_status = board.get_batch()\n",
        "                    hidden_i    = prev_status['hidden_state']\n",
        "                    cell_i      = prev_status['cell_state']\n",
        "                    h_t_tilde_i = prev_status['h_t_1_tilde']\n",
        "\n",
        "                    fab_input  += [y_hat_i]\n",
        "                    fab_hidden += [hidden_i]\n",
        "                    fab_cell   += [cell_i]\n",
        "                    fab_h_src  += [h_src[i, :, :]] * beam_size\n",
        "                    fab_mask   += [mask[i, :]] * beam_size\n",
        "                    if h_t_tilde_i is not None:\n",
        "                        fab_h_t_tilde += [h_t_tilde_i]\n",
        "                    else:\n",
        "                        fab_h_t_tilde = None\n",
        "\n",
        "            # Now, concatenate list of tensors.\n",
        "            fab_input  = torch.cat(fab_input,  dim=0)\n",
        "            fab_hidden = torch.cat(fab_hidden, dim=1)\n",
        "            fab_cell   = torch.cat(fab_cell,   dim=1)\n",
        "            fab_h_src  = torch.stack(fab_h_src)\n",
        "            fab_mask   = torch.stack(fab_mask)\n",
        "            if fab_h_t_tilde is not None:\n",
        "                fab_h_t_tilde = torch.cat(fab_h_t_tilde, dim=0)\n",
        "            # |fab_input|     = (current_batch_size, 1)\n",
        "            # |fab_hidden|    = (n_layers, current_batch_size, hidden_size)\n",
        "            # |fab_cell|      = (n_layers, current_batch_size, hidden_size)\n",
        "            # |fab_h_src|     = (current_batch_size, length, hidden_size)\n",
        "            # |fab_mask|      = (current_batch_size, length)\n",
        "            # |fab_h_t_tilde| = (current_batch_size, 1, hidden_size)\n",
        "\n",
        "            emb_t = self.emb_dec(fab_input)\n",
        "            # |emb_t| = (current_batch_size, 1, word_vec_size)\n",
        "\n",
        "            fab_decoder_output, (fab_hidden, fab_cell) = self.decoder(emb_t,\n",
        "                                                                      fab_h_t_tilde,\n",
        "                                                                      (fab_hidden, fab_cell))\n",
        "            # |fab_decoder_output| = (current_batch_size, 1, hidden_size)\n",
        "            context_vector = self.attn(fab_h_src, fab_decoder_output, fab_mask)\n",
        "            # |context_vector| = (current_batch_size, 1, hidden_size)\n",
        "            fab_h_t_tilde = self.tanh(self.concat(torch.cat([fab_decoder_output,\n",
        "                                                             context_vector\n",
        "                                                             ], dim=-1)))\n",
        "            # |fab_h_t_tilde| = (current_batch_size, 1, hidden_size)\n",
        "            y_hat = self.generator(fab_h_t_tilde)\n",
        "            # |y_hat| = (current_batch_size, 1, output_size)\n",
        "\n",
        "            # separate the result for each sample.\n",
        "            # fab_hidden[:, begin:end, :] = (n_layers, beam_size, hidden_size)\n",
        "            # fab_cell[:, begin:end, :]   = (n_layers, beam_size, hidden_size)\n",
        "            # fab_h_t_tilde[begin:end]    = (beam_size, 1, hidden_size)\n",
        "            cnt = 0\n",
        "            for board in boards:\n",
        "                if board.is_done() == 0:\n",
        "                    # Decide a range of each sample.\n",
        "                    begin = cnt * beam_size\n",
        "                    end = begin + beam_size\n",
        "\n",
        "                    # pick k-best results for each sample.\n",
        "                    board.collect_result(\n",
        "                        y_hat[begin:end],\n",
        "                        {\n",
        "                            'hidden_state': fab_hidden[:, begin:end, :],\n",
        "                            'cell_state'  : fab_cell[:, begin:end, :],\n",
        "                            'h_t_1_tilde' : fab_h_t_tilde[begin:end],\n",
        "                        },\n",
        "                    )\n",
        "                    cnt += 1\n",
        "\n",
        "            is_done = [board.is_done() for board in boards]\n",
        "            length += 1\n",
        "\n",
        "        # pick n-best hypothesis.\n",
        "        batch_sentences, batch_probs = [], []\n",
        "\n",
        "        # Collect the results.\n",
        "        for i, board in enumerate(boards):\n",
        "            sentences, probs = board.get_n_best(n_best, length_penalty=length_penalty)\n",
        "\n",
        "            batch_sentences += [sentences]\n",
        "            batch_probs     += [probs]\n",
        "\n",
        "        return batch_sentences, batch_probs\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1E8cueKIEtEp"
      },
      "source": [
        "# seq2seq\n",
        "  \n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pack_padded_sequence as pack\n",
        "from torch.nn.utils.rnn import pad_packed_sequence as unpack\n",
        "\n",
        "import simple_nmt.data_loader as data_loader\n",
        "from simple_nmt.search import SingleBeamSearchBoard\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden_size):\n",
        "        super(Attention, self).__init__()\n",
        "\n",
        "        self.linear = nn.Linear(hidden_size, hidden_size, bias=False)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, h_src, h_t_tgt, mask=None):\n",
        "        # |h_src| = (batch_size, length, hidden_size)\n",
        "        # |h_t_tgt| = (batch_size, 1, hidden_size)\n",
        "        # |mask| = (batch_size, length)\n",
        "\n",
        "        query = self.linear(h_t_tgt)\n",
        "        # |query| = (batch_size, 1, hidden_size)\n",
        "\n",
        "        weight = torch.bmm(query, h_src.transpose(1, 2))\n",
        "        # |weight| = (batch_size, 1, length)\n",
        "        if mask is not None:\n",
        "            # Set each weight as -inf, if the mask value equals to 1.\n",
        "            # Since the softmax operation makes -inf to 0,\n",
        "            # masked weights would be set to 0 after softmax operation.\n",
        "            # Thus, if the sample is shorter than other samples in mini-batch,\n",
        "            # the weight for empty time-step would be set to 0.\n",
        "            weight.masked_fill_(mask.unsqueeze(1), -float('inf'))\n",
        "        weight = self.softmax(weight)\n",
        "\n",
        "        context_vector = torch.bmm(weight, h_src)\n",
        "        # |context_vector| = (batch_size, 1, hidden_size)\n",
        "\n",
        "        return context_vector\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "\n",
        "    def __init__(self, word_vec_size, hidden_size, n_layers=4, dropout_p=.2):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        # Be aware of value of 'batch_first' parameter.\n",
        "        # Also, its hidden_size is half of original hidden_size,\n",
        "        # because it is bidirectional.\n",
        "        self.rnn = nn.LSTM(\n",
        "            word_vec_size,\n",
        "            int(hidden_size / 2),\n",
        "            num_layers=n_layers,\n",
        "            dropout=dropout_p,\n",
        "            bidirectional=True,\n",
        "            batch_first=True,\n",
        "        )\n",
        "\n",
        "    def forward(self, emb):\n",
        "        # |emb| = (batch_size, length, word_vec_size)\n",
        "\n",
        "        if isinstance(emb, tuple):\n",
        "            x, lengths = emb\n",
        "            x = pack(x, lengths.tolist(), batch_first=True)\n",
        "\n",
        "            # Below is how pack_padded_sequence works.\n",
        "            # As you can see,\n",
        "            # PackedSequence object has information about mini-batch-wise information,\n",
        "            # not time-step-wise information.\n",
        "            # \n",
        "            # a = [torch.tensor([1,2,3]), torch.tensor([3,4])]\n",
        "            # b = torch.nn.utils.rnn.pad_sequence(a, batch_first=True)\n",
        "            # >>>>\n",
        "            # tensor([[ 1,  2,  3],\n",
        "            #     [ 3,  4,  0]])\n",
        "            # torch.nn.utils.rnn.pack_padded_sequence(b, batch_first=True, lengths=[3,2]\n",
        "            # >>>>PackedSequence(data=tensor([ 1,  3,  2,  4,  3]), batch_sizes=tensor([ 2,  2,  1]))\n",
        "        else:\n",
        "            x = emb\n",
        "\n",
        "        y, h = self.rnn(x)\n",
        "        # |y| = (batch_size, length, hidden_size)\n",
        "        # |h[0]| = (num_layers * 2, batch_size, hidden_size / 2)\n",
        "\n",
        "        if isinstance(emb, tuple):\n",
        "            y, _ = unpack(y, batch_first=True)\n",
        "\n",
        "        return y, h\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "\n",
        "    def __init__(self, word_vec_size, hidden_size, n_layers=4, dropout_p=.2):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        # Be aware of value of 'batch_first' parameter and 'bidirectional' parameter.\n",
        "        self.rnn = nn.LSTM(\n",
        "            word_vec_size + hidden_size,\n",
        "            hidden_size,\n",
        "            num_layers=n_layers,\n",
        "            dropout=dropout_p,\n",
        "            bidirectional=False,\n",
        "            batch_first=True,\n",
        "        )\n",
        "\n",
        "    def forward(self, emb_t, h_t_1_tilde, h_t_1):\n",
        "        # |emb_t| = (batch_size, 1, word_vec_size)\n",
        "        # |h_t_1_tilde| = (batch_size, 1, hidden_size)\n",
        "        # |h_t_1[0]| = (n_layers, batch_size, hidden_size)\n",
        "        batch_size = emb_t.size(0)\n",
        "        hidden_size = h_t_1[0].size(-1)\n",
        "\n",
        "        if h_t_1_tilde is None:\n",
        "            # If this is the first time-step,\n",
        "            h_t_1_tilde = emb_t.new(batch_size, 1, hidden_size).zero_()\n",
        "\n",
        "        # Input feeding trick.\n",
        "        x = torch.cat([emb_t, h_t_1_tilde], dim=-1)\n",
        "\n",
        "        # Unlike encoder, decoder must take an input for sequentially.\n",
        "        y, h = self.rnn(x, h_t_1)\n",
        "\n",
        "        return y, h\n",
        "\n",
        "\n",
        "class Generator(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        self.output = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=-1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # |x| = (batch_size, length, hidden_size)\n",
        "\n",
        "        y = self.softmax(self.output(x))\n",
        "        # |y| = (batch_size, length, output_size)\n",
        "\n",
        "        # Return log-probability instead of just probability.\n",
        "        return y\n",
        "\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size,\n",
        "        word_vec_size,\n",
        "        hidden_size,\n",
        "        output_size,\n",
        "        n_layers=4,\n",
        "        dropout_p=.2\n",
        "    ):\n",
        "        self.input_size = input_size\n",
        "        self.word_vec_size = word_vec_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.dropout_p = dropout_p\n",
        "\n",
        "        super(Seq2Seq, self).__init__()\n",
        "\n",
        "        self.emb_src = nn.Embedding(input_size, word_vec_size)\n",
        "        self.emb_dec = nn.Embedding(output_size, word_vec_size)\n",
        "\n",
        "        self.encoder = Encoder(\n",
        "            word_vec_size, hidden_size,\n",
        "            n_layers=n_layers, dropout_p=dropout_p,\n",
        "        )\n",
        "        self.decoder = Decoder(\n",
        "            word_vec_size, hidden_size,\n",
        "            n_layers=n_layers, dropout_p=dropout_p,\n",
        "        )\n",
        "        self.attn = Attention(hidden_size)\n",
        "\n",
        "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.generator = Generator(hidden_size, output_size)\n",
        "\n",
        "    def generate_mask(self, x, length):\n",
        "        mask = []\n",
        "\n",
        "        max_length = max(length)\n",
        "        for l in length:\n",
        "            if max_length - l > 0:\n",
        "                # If the length is shorter than maximum length among samples, \n",
        "                # set last few values to be 1s to remove attention weight.\n",
        "                mask += [torch.cat([x.new_ones(1, l).zero_(),\n",
        "                                    x.new_ones(1, (max_length - l))\n",
        "                                    ], dim=-1)]\n",
        "            else:\n",
        "                # If the length of the sample equals to maximum length among samples, \n",
        "                # set every value in mask to be 0.\n",
        "                mask += [x.new_ones(1, l).zero_()]\n",
        "\n",
        "        mask = torch.cat(mask, dim=0).bool()\n",
        "\n",
        "        return mask\n",
        "\n",
        "    def merge_encoder_hiddens(self, encoder_hiddens):\n",
        "        new_hiddens = []\n",
        "        new_cells = []\n",
        "\n",
        "        hiddens, cells = encoder_hiddens\n",
        "\n",
        "        # i-th and (i+1)-th layer is opposite direction.\n",
        "        # Also, each direction of layer is half hidden size.\n",
        "        # Therefore, we concatenate both directions to 1 hidden size layer.\n",
        "        for i in range(0, hiddens.size(0), 2):\n",
        "            new_hiddens += [torch.cat([hiddens[i], hiddens[i + 1]], dim=-1)]\n",
        "            new_cells += [torch.cat([cells[i], cells[i + 1]], dim=-1)]\n",
        "\n",
        "        new_hiddens, new_cells = torch.stack(new_hiddens), torch.stack(new_cells)\n",
        "\n",
        "        return (new_hiddens, new_cells)\n",
        "\n",
        "    def fast_merge_encoder_hiddens(self, encoder_hiddens):\n",
        "        # Merge bidirectional to uni-directional\n",
        "        # We need to convert size from (n_layers * 2, batch_size, hidden_size / 2)\n",
        "        # to (n_layers, batch_size, hidden_size).\n",
        "        # Thus, the converting operation will not working with just 'view' method.\n",
        "        h_0_tgt, c_0_tgt = encoder_hiddens\n",
        "        batch_size = h_0_tgt.size(1)\n",
        "\n",
        "        h_0_tgt = h_0_tgt.transpose(0, 1).contiguous().view(batch_size,\n",
        "                                                            -1,\n",
        "                                                            self.hidden_size\n",
        "                                                            ).transpose(0, 1).contiguous()\n",
        "        c_0_tgt = c_0_tgt.transpose(0, 1).contiguous().view(batch_size,\n",
        "                                                            -1,\n",
        "                                                            self.hidden_size\n",
        "                                                            ).transpose(0, 1).contiguous()\n",
        "        # You can use 'merge_encoder_hiddens' method, instead of using above 3 lines.\n",
        "        # 'merge_encoder_hiddens' method works with non-parallel way.\n",
        "        # h_0_tgt = self.merge_encoder_hiddens(h_0_tgt)\n",
        "\n",
        "        # |h_src| = (batch_size, length, hidden_size)\n",
        "        # |h_0_tgt| = (n_layers, batch_size, hidden_size)\n",
        "        return h_0_tgt, c_0_tgt\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        batch_size = tgt.size(0)\n",
        "\n",
        "        mask = None\n",
        "        x_length = None\n",
        "        if isinstance(src, tuple):\n",
        "            x, x_length = src\n",
        "            # Based on the length information, gererate mask to prevent that\n",
        "            # shorter sample has wasted attention.\n",
        "            mask = self.generate_mask(x, x_length)\n",
        "            # |mask| = (batch_size, length)\n",
        "        else:\n",
        "            x = src\n",
        "\n",
        "        if isinstance(tgt, tuple):\n",
        "            tgt = tgt[0]\n",
        "\n",
        "        # Get word embedding vectors for every time-step of input sentence.\n",
        "        emb_src = self.emb_src(x)\n",
        "        # |emb_src| = (batch_size, length, word_vec_size)\n",
        "\n",
        "        # The last hidden state of the encoder would be a initial hidden state of decoder.\n",
        "        h_src, h_0_tgt = self.encoder((emb_src, x_length))\n",
        "        # |h_src| = (batch_size, length, hidden_size)\n",
        "        # |h_0_tgt| = (n_layers * 2, batch_size, hidden_size / 2)\n",
        "\n",
        "        h_0_tgt = self.fast_merge_encoder_hiddens(h_0_tgt)\n",
        "        emb_tgt = self.emb_dec(tgt)\n",
        "        # |emb_tgt| = (batch_size, length, word_vec_size)\n",
        "        h_tilde = []\n",
        "\n",
        "        h_t_tilde = None\n",
        "        decoder_hidden = h_0_tgt\n",
        "        # Run decoder until the end of the time-step.\n",
        "        for t in range(tgt.size(1)):\n",
        "            # Teacher Forcing: take each input from training set,\n",
        "            # not from the last time-step's output.\n",
        "            # Because of Teacher Forcing,\n",
        "            # training procedure and inference procedure becomes different.\n",
        "            # Of course, because of sequential running in decoder,\n",
        "            # this causes severe bottle-neck.\n",
        "            emb_t = emb_tgt[:, t, :].unsqueeze(1)\n",
        "            # |emb_t| = (batch_size, 1, word_vec_size)\n",
        "            # |h_t_tilde| = (batch_size, 1, hidden_size)\n",
        "\n",
        "            decoder_output, decoder_hidden = self.decoder(emb_t,\n",
        "                                                          h_t_tilde,\n",
        "                                                          decoder_hidden\n",
        "                                                          )\n",
        "            # |decoder_output| = (batch_size, 1, hidden_size)\n",
        "            # |decoder_hidden| = (n_layers, batch_size, hidden_size)\n",
        "\n",
        "            context_vector = self.attn(h_src, decoder_output, mask)\n",
        "            # |context_vector| = (batch_size, 1, hidden_size)\n",
        "\n",
        "            h_t_tilde = self.tanh(self.concat(torch.cat([decoder_output,\n",
        "                                                         context_vector\n",
        "                                                         ], dim=-1)))\n",
        "            # |h_t_tilde| = (batch_size, 1, hidden_size)\n",
        "\n",
        "            h_tilde += [h_t_tilde]\n",
        "\n",
        "        h_tilde = torch.cat(h_tilde, dim=1)\n",
        "        # |h_tilde| = (batch_size, length, hidden_size)\n",
        "\n",
        "        y_hat = self.generator(h_tilde)\n",
        "        # |y_hat| = (batch_size, length, output_size)\n",
        "\n",
        "        return y_hat\n",
        "\n",
        "    def search(self, src, is_greedy=True, max_length=255):\n",
        "        if isinstance(src, tuple):\n",
        "            x, x_length = src\n",
        "            mask = self.generate_mask(x, x_length)\n",
        "        else:\n",
        "            x, x_length = src, None\n",
        "            mask = None\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # Same procedure as teacher forcing.\n",
        "        emb_src = self.emb_src(x)\n",
        "        h_src, h_0_tgt = self.encoder((emb_src, x_length))\n",
        "        decoder_hidden = self.fast_merge_encoder_hiddens(h_0_tgt)\n",
        "\n",
        "        # Fill a vector, which has 'batch_size' dimension, with BOS value.\n",
        "        y = x.new(batch_size, 1).zero_() + data_loader.BOS\n",
        "\n",
        "        is_decoding = x.new_ones(batch_size, 1).bool()\n",
        "        h_t_tilde, y_hats, indice = None, [], []\n",
        "        \n",
        "        # Repeat a loop while sum of 'is_decoding' flag is bigger than 0,\n",
        "        # or current time-step is smaller than maximum length.\n",
        "        while is_decoding.sum() > 0 and len(indice) < max_length:\n",
        "            # Unlike training procedure,\n",
        "            # take the last time-step's output during the inference.\n",
        "            emb_t = self.emb_dec(y)\n",
        "            # |emb_t| = (batch_size, 1, word_vec_size)\n",
        "\n",
        "            decoder_output, decoder_hidden = self.decoder(emb_t,\n",
        "                                                          h_t_tilde,\n",
        "                                                          decoder_hidden)\n",
        "            context_vector = self.attn(h_src, decoder_output, mask)\n",
        "            h_t_tilde = self.tanh(self.concat(torch.cat([decoder_output,\n",
        "                                                         context_vector\n",
        "                                                         ], dim=-1)))\n",
        "            y_hat = self.generator(h_t_tilde)\n",
        "            # |y_hat| = (batch_size, 1, output_size)\n",
        "            y_hats += [y_hat]\n",
        "\n",
        "            if is_greedy:\n",
        "                y = y_hat.argmax(dim=-1)\n",
        "                # |y| = (batch_size, 1)\n",
        "            else:\n",
        "                # Take a random sampling based on the multinoulli distribution.\n",
        "                y = torch.multinomial(y_hat.exp().view(batch_size, -1), 1)\n",
        "                # |y| = (batch_size, 1)\n",
        "\n",
        "            # Put PAD if the sample is done.\n",
        "            y = y.masked_fill_(~is_decoding, data_loader.PAD)\n",
        "            # Update is_decoding if there is EOS token.\n",
        "            is_decoding = is_decoding * torch.ne(y, data_loader.EOS)\n",
        "            # |is_decoding| = (batch_size, 1)\n",
        "            indice += [y]\n",
        "\n",
        "        y_hats = torch.cat(y_hats, dim=1)\n",
        "        indice = torch.cat(indice, dim=1)\n",
        "        # |y_hat| = (batch_size, length, output_size)\n",
        "        # |indice| = (batch_size, length)\n",
        "\n",
        "        return y_hats, indice\n",
        "\n",
        "    #rofile\n",
        "    def batch_beam_search(\n",
        "        self,\n",
        "        src,\n",
        "        beam_size=5,\n",
        "        max_length=255,\n",
        "        n_best=1,\n",
        "        length_penalty=.2\n",
        "    ):\n",
        "        mask, x_length = None, None\n",
        "\n",
        "        if isinstance(src, tuple):\n",
        "            x, x_length = src\n",
        "            mask = self.generate_mask(x, x_length)\n",
        "            # |mask| = (batch_size, length)\n",
        "        else:\n",
        "            x = src\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        emb_src = self.emb_src(x)\n",
        "        h_src, h_0_tgt = self.encoder((emb_src, x_length))\n",
        "        # |h_src| = (batch_size, length, hidden_size)\n",
        "        h_0_tgt = self.fast_merge_encoder_hiddens(h_0_tgt)\n",
        "\n",
        "        # initialize 'SingleBeamSearchBoard' as many as batch_size\n",
        "        boards = [SingleBeamSearchBoard(\n",
        "            h_src.device,\n",
        "            {\n",
        "                'hidden_state': {\n",
        "                    'init_status': h_0_tgt[0][:, i, :].unsqueeze(1),\n",
        "                    'batch_dim_index': 1,\n",
        "                }, # |hidden_state| = (n_layers, batch_size, hidden_size)\n",
        "                'cell_state': {\n",
        "                    'init_status': h_0_tgt[1][:, i, :].unsqueeze(1),\n",
        "                    'batch_dim_index': 1,\n",
        "                }, # |cell_state| = (n_layers, batch_size, hidden_size)\n",
        "                'h_t_1_tilde': {\n",
        "                    'init_status': None,\n",
        "                    'batch_dim_index': 0,\n",
        "                }, # |h_t_1_tilde| = (batch_size, 1, hidden_size)\n",
        "            },\n",
        "            beam_size=beam_size,\n",
        "            max_length=max_length,\n",
        "        ) for i in range(batch_size)]\n",
        "        is_done = [board.is_done() for board in boards]\n",
        "\n",
        "        length = 0\n",
        "        # Run loop while sum of 'is_done' is smaller than batch_size, \n",
        "        # or length is still smaller than max_length.\n",
        "        while sum(is_done) < batch_size and length <= max_length:\n",
        "            # current_batch_size = sum(is_done) * beam_size\n",
        "\n",
        "            # Initialize fabricated variables.\n",
        "            # As far as batch-beam-search is running, \n",
        "            # temporary batch-size for fabricated mini-batch is \n",
        "            # 'beam_size'-times bigger than original batch_size.\n",
        "            fab_input, fab_hidden, fab_cell, fab_h_t_tilde = [], [], [], []\n",
        "            fab_h_src, fab_mask = [], []\n",
        "            \n",
        "            # Build fabricated mini-batch in non-parallel way.\n",
        "            # This may cause a bottle-neck.\n",
        "            for i, board in enumerate(boards):\n",
        "                # Batchify if the inference for the sample is still not finished.\n",
        "                if board.is_done() == 0:\n",
        "                    y_hat_i, prev_status = board.get_batch()\n",
        "                    hidden_i    = prev_status['hidden_state']\n",
        "                    cell_i      = prev_status['cell_state']\n",
        "                    h_t_tilde_i = prev_status['h_t_1_tilde']\n",
        "\n",
        "                    fab_input  += [y_hat_i]\n",
        "                    fab_hidden += [hidden_i]\n",
        "                    fab_cell   += [cell_i]\n",
        "                    fab_h_src  += [h_src[i, :, :]] * beam_size\n",
        "                    fab_mask   += [mask[i, :]] * beam_size\n",
        "                    if h_t_tilde_i is not None:\n",
        "                        fab_h_t_tilde += [h_t_tilde_i]\n",
        "                    else:\n",
        "                        fab_h_t_tilde = None\n",
        "\n",
        "            # Now, concatenate list of tensors.\n",
        "            fab_input  = torch.cat(fab_input,  dim=0)\n",
        "            fab_hidden = torch.cat(fab_hidden, dim=1)\n",
        "            fab_cell   = torch.cat(fab_cell,   dim=1)\n",
        "            fab_h_src  = torch.stack(fab_h_src)\n",
        "            fab_mask   = torch.stack(fab_mask)\n",
        "            if fab_h_t_tilde is not None:\n",
        "                fab_h_t_tilde = torch.cat(fab_h_t_tilde, dim=0)\n",
        "            # |fab_input|     = (current_batch_size, 1)\n",
        "            # |fab_hidden|    = (n_layers, current_batch_size, hidden_size)\n",
        "            # |fab_cell|      = (n_layers, current_batch_size, hidden_size)\n",
        "            # |fab_h_src|     = (current_batch_size, length, hidden_size)\n",
        "            # |fab_mask|      = (current_batch_size, length)\n",
        "            # |fab_h_t_tilde| = (current_batch_size, 1, hidden_size)\n",
        "\n",
        "            emb_t = self.emb_dec(fab_input)\n",
        "            # |emb_t| = (current_batch_size, 1, word_vec_size)\n",
        "\n",
        "            fab_decoder_output, (fab_hidden, fab_cell) = self.decoder(emb_t,\n",
        "                                                                      fab_h_t_tilde,\n",
        "                                                                      (fab_hidden, fab_cell))\n",
        "            # |fab_decoder_output| = (current_batch_size, 1, hidden_size)\n",
        "            context_vector = self.attn(fab_h_src, fab_decoder_output, fab_mask)\n",
        "            # |context_vector| = (current_batch_size, 1, hidden_size)\n",
        "            fab_h_t_tilde = self.tanh(self.concat(torch.cat([fab_decoder_output,\n",
        "                                                             context_vector\n",
        "                                                             ], dim=-1)))\n",
        "            # |fab_h_t_tilde| = (current_batch_size, 1, hidden_size)\n",
        "            y_hat = self.generator(fab_h_t_tilde)\n",
        "            # |y_hat| = (current_batch_size, 1, output_size)\n",
        "\n",
        "            # separate the result for each sample.\n",
        "            # fab_hidden[:, begin:end, :] = (n_layers, beam_size, hidden_size)\n",
        "            # fab_cell[:, begin:end, :]   = (n_layers, beam_size, hidden_size)\n",
        "            # fab_h_t_tilde[begin:end]    = (beam_size, 1, hidden_size)\n",
        "            cnt = 0\n",
        "            for board in boards:\n",
        "                if board.is_done() == 0:\n",
        "                    # Decide a range of each sample.\n",
        "                    begin = cnt * beam_size\n",
        "                    end = begin + beam_size\n",
        "\n",
        "                    # pick k-best results for each sample.\n",
        "                    board.collect_result(\n",
        "                        y_hat[begin:end],\n",
        "                        {\n",
        "                            'hidden_state': fab_hidden[:, begin:end, :],\n",
        "                            'cell_state'  : fab_cell[:, begin:end, :],\n",
        "                            'h_t_1_tilde' : fab_h_t_tilde[begin:end],\n",
        "                        },\n",
        "                    )\n",
        "                    cnt += 1\n",
        "\n",
        "            is_done = [board.is_done() for board in boards]\n",
        "            length += 1\n",
        "\n",
        "        # pick n-best hypothesis.\n",
        "        batch_sentences, batch_probs = [], []\n",
        "\n",
        "        # Collect the results.\n",
        "        for i, board in enumerate(boards):\n",
        "            sentences, probs = board.get_n_best(n_best, length_penalty=length_penalty)\n",
        "\n",
        "            batch_sentences += [sentences]\n",
        "            batch_probs     += [probs]\n",
        "\n",
        "        return batch_sentences, batch_probs\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlvBk6MIeob3"
      },
      "source": [
        "#Transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRljaek5enAl"
      },
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import simple_nmt.data_loader as data_loader\n",
        "from simple_nmt.search import SingleBeamSearchBoard\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, Q, K, V, mask=None, dk=64):\n",
        "        # |Q| = (batch_size, m, hidden_size)\n",
        "        # |K| = |V| = (batch_size, n, hidden_size)\n",
        "        # |mask| = (batch_size, m, n) \n",
        "\n",
        "        w = torch.bmm(Q, K.transpose(1, 2))\n",
        "        # |w| = (batch_size, m, n)\n",
        "        if mask is not None:\n",
        "            assert w.size() == mask.size()\n",
        "            w.masked_fill_(mask, -float('inf'))\n",
        "\n",
        "        w = self.softmax(w / (dk**.5))\n",
        "        c = torch.bmm(w, V)\n",
        "        # |c| = (batch_size, m, hidden_size) #모든 타임스텝을 한번에 넣어 어텐션 스코어 구함\n",
        "\n",
        "        return c\n",
        "\n",
        "\n",
        "class MultiHead(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden_size, n_splits):\n",
        "        super().__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_splits = n_splits\n",
        "\n",
        "        # Note that we don't have to declare each linear layer, separately.\n",
        "        self.Q_linear = nn.Linear(hidden_size, hidden_size, bias=False)\n",
        "        self.K_linear = nn.Linear(hidden_size, hidden_size, bias=False)\n",
        "        self.V_linear = nn.Linear(hidden_size, hidden_size, bias=False)\n",
        "        self.linear = nn.Linear(hidden_size, hidden_size, bias=False)\n",
        "\n",
        "        self.attn = Attention()\n",
        "\n",
        "    def forward(self, Q, K, V, mask=None): #decoder to encoder \n",
        "        # |Q|    = (batch_size, m, hidden_size)\n",
        "        # |K|    = (batch_size, n, hidden_size)\n",
        "        # |V|    = |K|\n",
        "        # |mask| = (batch_size, m, n)\n",
        "\n",
        "        QWs = self.Q_linear(Q).split(self.hidden_size // self.n_splits, dim=-1) #하나의 리스트로 #내가 원하는 크기고 내가 원하는 차원에서 스플릿\n",
        "        KWs = self.K_linear(K).split(self.hidden_size // self.n_splits, dim=-1)\n",
        "        VWs = self.V_linear(V).split(self.hidden_size // self.n_splits, dim=-1)\n",
        "        # |QW_i| = (batch_size, m, hidden_size / n_splits)\n",
        "        # |KW_i| = |VW_i| = (batch_size, n, hidden_size / n_splits)\n",
        "\n",
        "        # By concatenating splited linear transformed results,\n",
        "        # we can remove sequential operations,\n",
        "        # like mini-batch parallel operations.\n",
        "        QWs = torch.cat(QWs, dim=0)\n",
        "        KWs = torch.cat(KWs, dim=0)\n",
        "        VWs = torch.cat(VWs, dim=0)\n",
        "        # |QWs| = (batch_size * n_splits, m, hidden_size / n_splits)\n",
        "        # |KWs| = |VWs| = (batch_size * n_splits, n, hidden_size / n_splits)\n",
        "\n",
        "        if mask is not None:\n",
        "            mask = torch.cat([mask for _ in range(self.n_splits)], dim=0) #0번 차원에 대해 합쳐라\n",
        "            # |mask| = (batch_size * n_splits, m, n)\n",
        "\n",
        "        c = self.attn(\n",
        "            QWs, KWs, VWs,\n",
        "            mask=mask,\n",
        "            dk=self.hidden_size // self.n_splits,\n",
        "        )\n",
        "        # |c| = (batch_size * n_splits, m, hidden_size / n_splits)\n",
        "\n",
        "        # We need to restore temporal mini-batchfied multi-head attention results.\n",
        "        c = c.split(Q.size(0), dim=0)\n",
        "        # |c_i| = (batch_size, m, hidden_size / n_splits)\n",
        "        c = self.linear(torch.cat(c, dim=-1))\n",
        "        # |c| = (batch_size, m, hidden_size) #속도의 저하 막기 위한 작업\n",
        "\n",
        "        return c\n",
        "\n",
        "\n",
        "class EncoderBlock(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        hidden_size,\n",
        "        n_splits,\n",
        "        dropout_p=.1,\n",
        "        use_leaky_relu=False,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.attn = MultiHead(hidden_size, n_splits)\n",
        "        self.attn_norm = nn.LayerNorm(hidden_size)\n",
        "        self.attn_dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(hidden_size, hidden_size * 4),\n",
        "            nn.LeakyReLU() if use_leaky_relu else nn.ReLU(),\n",
        "            nn.Linear(hidden_size * 4, hidden_size),\n",
        "        )\n",
        "        self.fc_norm = nn.LayerNorm(hidden_size)\n",
        "        self.fc_dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        # |x|    = (batch_size, n, hidden_size)\n",
        "        # |mask| = (batch_size, n, n)\n",
        "\n",
        "        # Post-LN:\n",
        "        # z = self.attn_norm(x + self.attn_dropout(self.attn(Q=x,\n",
        "        #                                                    K=x,\n",
        "        #                                                    V=x,\n",
        "        #                                                    mask=mask)))\n",
        "        # z = self.fc_norm(z + self.fc_dropout(self.fc(z)))\n",
        "\n",
        "        # Pre-LN:\n",
        "        z = self.attn_norm(x)\n",
        "        z = x + self.attn_dropout(self.attn(Q=z,\n",
        "                                            K=z,\n",
        "                                            V=z,\n",
        "                                            mask=mask))\n",
        "        z = z + self.fc_dropout(self.fc(self.fc_norm(z)))\n",
        "        # |z| = (batch_size, n, hidden_size)\n",
        "\n",
        "        return z, mask\n",
        "\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        hidden_size,\n",
        "        n_splits,\n",
        "        dropout_p=.1,\n",
        "        use_leaky_relu=False,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.masked_attn = MultiHead(hidden_size, n_splits)\n",
        "        self.masked_attn_norm = nn.LayerNorm(hidden_size)\n",
        "        self.masked_attn_dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "        self.attn = MultiHead(hidden_size, n_splits)\n",
        "        self.attn_norm = nn.LayerNorm(hidden_size)\n",
        "        self.attn_dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(hidden_size, hidden_size * 4),\n",
        "            nn.LeakyReLU() if use_leaky_relu else nn.ReLU(),\n",
        "            nn.Linear(hidden_size * 4, hidden_size),\n",
        "        )\n",
        "        self.fc_norm = nn.LayerNorm(hidden_size)\n",
        "        self.fc_dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "    def forward(self, x, key_and_value, mask, prev, future_mask):\n",
        "      #학습모델에서는 모든 타임스텝이 한번에 처리되지만, 추론모델에서는 타임스텝별로 진행. 이전타임스텝의 결과를 기다려야 함\n",
        "        # |key_and_value| = (batch_size, n, hidden_size)\n",
        "        # |mask|          = (batch_size, m, n) #pad mask\n",
        "\n",
        "        # In case of inference, we don't have to repeat same feed-forward operations.\n",
        "        # Thus, we save previous feed-forward results.\n",
        "        if prev is None: # Training mode  #학습모드와 추론모드 구분\n",
        "            # |x|           = (batch_size, m, hidden_size) #모든 타임스텝 다 들어옴\n",
        "            # |prev|        = None\n",
        "            # |future_mask| = (batch_size, m, m) #미래의 정보를 보지 못하게! m*m\n",
        "            # |z|           = (batch_size, m, hidden_size) #결과물\n",
        "\n",
        "            # Post-LN:\n",
        "            # z = self.masked_attn_norm(x + self.masked_attn_dropout(    #차원이 똑같아서 +을 할 수 있다\n",
        "            #     self.masked_attn(x, x, x, mask=future_mask) #셀프 어텐션이니까 x, x, x\n",
        "            # ))\n",
        "\n",
        "            # Pre-LN:\n",
        "            z = self.masked_attn_norm(x)\n",
        "            z = x + self.masked_attn_dropout(\n",
        "                self.masked_attn(z, z, z, mask=future_mask)\n",
        "            )\n",
        "        else: # Inference mode\n",
        "            # |x|           = (batch_size, 1, hidden_size)\n",
        "            # |prev|        = (batch_size, t - 1, hidden_size)\n",
        "            # |future_mask| = None\n",
        "            # |z|           = (batch_size, 1, hidden_size)\n",
        "\n",
        "            # Post-LN:\n",
        "            # z = self.masked_attn_norm(x + self.masked_attn_dropout(\n",
        "            #     self.masked_attn(x, prev, prev, mask=None)\n",
        "            # ))\n",
        "\n",
        "            # Pre-LN:\n",
        "            normed_prev = self.masked_attn_norm(prev)\n",
        "            z = self.masked_attn_norm(x)\n",
        "            z = x + self.masked_attn_dropout(\n",
        "                self.masked_attn(z, normed_prev, normed_prev, mask=None) #추론모드라 미래의 정보가 없음\n",
        "            )\n",
        "\n",
        "        # Post-LN:\n",
        "        # z = self.attn_norm(z + self.attn_dropout(self.attn(Q=z,\n",
        "        #                                                    K=key_and_value,\n",
        "        #                                                    V=key_and_value,\n",
        "        #                                                    mask=mask)))\n",
        "\n",
        "        # Pre-LN:\n",
        "        normed_key_and_value = self.attn_norm(key_and_value)\n",
        "        z = z + self.attn_dropout(self.attn(Q=self.attn_norm(z),\n",
        "                                            K=normed_key_and_value,\n",
        "                                            V=normed_key_and_value,\n",
        "                                            mask=mask))\n",
        "        # |z| = (batch_size, m, hidden_size)\n",
        "\n",
        "        # Post-LN:\n",
        "        # z = self.fc_norm(z + self.fc_dropout(self.fc(z)))\n",
        "\n",
        "        # Pre-LN:\n",
        "        z = z + self.fc_dropout(self.fc(self.fc_norm(z)))\n",
        "        # |z| = (batch_size, m, hidden_size)\n",
        "\n",
        "        return z, key_and_value, mask, prev, future_mask\n",
        "\n",
        "\n",
        "class MySequential(nn.Sequential):\n",
        "\n",
        "    def forward(self, *x):\n",
        "        # nn.Sequential class does not provide multiple input arguments and returns.\n",
        "        # Thus, we need to define new class to solve this issue.\n",
        "        # Note that each block has same function interface.\n",
        "\n",
        "        for module in self._modules.values():\n",
        "            x = module(*x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size,\n",
        "        hidden_size,\n",
        "        output_size,\n",
        "        n_splits,\n",
        "        n_enc_blocks=6,\n",
        "        n_dec_blocks=6,\n",
        "        dropout_p=.1,\n",
        "        use_leaky_relu=False,\n",
        "        max_length=512,\n",
        "    ):\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.n_splits = n_splits\n",
        "        self.n_enc_blocks = n_enc_blocks\n",
        "        self.n_dec_blocks = n_dec_blocks\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.emb_enc = nn.Embedding(input_size, hidden_size) #인코더 임베딩\n",
        "        self.emb_dec = nn.Embedding(output_size, hidden_size) #디코더 임베딩\n",
        "        self.emb_dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "        self.pos_enc = self._generate_pos_enc(hidden_size, max_length) #위치정보 #큰 걸 만들어 놓고 잘라서 씀\n",
        "\n",
        "        self.encoder = MySequential(\n",
        "            *[EncoderBlock(\n",
        "                hidden_size,\n",
        "                n_splits,\n",
        "                dropout_p,\n",
        "                use_leaky_relu,\n",
        "              ) for _ in range(n_enc_blocks)],\n",
        "        )\n",
        "        self.decoder = MySequential(\n",
        "            *[DecoderBlock(\n",
        "                hidden_size,\n",
        "                n_splits,\n",
        "                dropout_p,\n",
        "                use_leaky_relu,\n",
        "              ) for _ in range(n_dec_blocks)],\n",
        "        )\n",
        "        self.generator = nn.Sequential(\n",
        "            nn.LayerNorm(hidden_size), # Only for Pre-LN Transformer. #빼먹으면 안 됨\n",
        "            nn.Linear(hidden_size, output_size),\n",
        "            nn.LogSoftmax(dim=-1),\n",
        "        )\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def _generate_pos_enc(self, hidden_size, max_length):\n",
        "        enc = torch.FloatTensor(max_length, hidden_size).zero_()\n",
        "        # |enc| = (max_length, hidden_size)\n",
        "\n",
        "        pos = torch.arange(0, max_length).unsqueeze(-1).float()\n",
        "        dim = torch.arange(0, hidden_size // 2).unsqueeze(0).float()\n",
        "        # |pos| = (max_length, 1)\n",
        "        # |dim| = (1, hidden_size // 2)\n",
        "\n",
        "        enc[:, 0::2] = torch.sin(pos / 1e+4**dim.div(float(hidden_size)))\n",
        "        enc[:, 1::2] = torch.cos(pos / 1e+4**dim.div(float(hidden_size)))\n",
        "\n",
        "        return enc\n",
        "\n",
        "    def _position_encoding(self, x, init_pos=0):\n",
        "        # |x| = (batch_size, n, hidden_size)\n",
        "        # |self.pos_enc| = (max_length, hidden_size)\n",
        "        assert x.size(-1) == self.pos_enc.size(-1)\n",
        "        assert x.size(1) + init_pos <= self.max_length\n",
        "\n",
        "        pos_enc = self.pos_enc[init_pos:init_pos + x.size(1)].unsqueeze(0)\n",
        "        # |pos_enc| = (1, n, hidden_size)\n",
        "        x = x + pos_enc.to(x.device)\n",
        "\n",
        "        return x\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def _generate_mask(self, x, length):\n",
        "        mask = []\n",
        "\n",
        "        max_length = max(length)\n",
        "        for l in length:\n",
        "            if max_length - l > 0:\n",
        "                # If the length is shorter than maximum length among samples,\n",
        "                # set last few values to be 1s to remove attention weight.\n",
        "                mask += [torch.cat([x.new_ones(1, l).zero_(),\n",
        "                                    x.new_ones(1, (max_length - l))\n",
        "                                    ], dim=-1)]\n",
        "            else:\n",
        "                # If length of sample equals to maximum length among samples,\n",
        "                # set every value in mask to be 0.\n",
        "                mask += [x.new_ones(1, l).zero_()]\n",
        "\n",
        "        mask = torch.cat(mask, dim=0).bool()\n",
        "        # |mask| = (batch_size, max_length)\n",
        "\n",
        "        return mask\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        # |x[0]| = (batch_size, n)\n",
        "        # |y|    = (batch_size, m)\n",
        "\n",
        "        # Mask to prevent having attention weight on padding position.\n",
        "        with torch.no_grad():\n",
        "            mask = self._generate_mask(x[0], x[1])\n",
        "            # |mask| = (batch_size, n)\n",
        "            x = x[0]\n",
        "\n",
        "            mask_enc = mask.unsqueeze(1).expand(*x.size(), mask.size(-1))\n",
        "            mask_dec = mask.unsqueeze(1).expand(*y.size(), mask.size(-1))\n",
        "            # |mask_enc| = (batch_size, n, n)\n",
        "            # |mask_dec| = (batch_size, m, n)\n",
        "\n",
        "        z = self.emb_dropout(self._position_encoding(self.emb_enc(x)))\n",
        "        z, _ = self.encoder(z, mask_enc)\n",
        "        # |z| = (batch_size, n, hidden_size)\n",
        "\n",
        "        # Generate future mask\n",
        "        with torch.no_grad():\n",
        "            future_mask = torch.triu(x.new_ones((y.size(1), y.size(1))), diagonal=1).bool()\n",
        "            # |future_mask| = (m, m)\n",
        "            future_mask = future_mask.unsqueeze(0).expand(y.size(0), *future_mask.size())\n",
        "            # |fwd_mask| = (batch_size, m, m)\n",
        "\n",
        "        h = self.emb_dropout(self._position_encoding(self.emb_dec(y)))\n",
        "        h, _, _, _, _ = self.decoder(h, z, mask_dec, None, future_mask)\n",
        "        # |h| = (batch_size, m, hidden_size)\n",
        "\n",
        "        y_hat = self.generator(h)\n",
        "        # |y_hat| = (batch_size, m, output_size)\n",
        "\n",
        "        return y_hat\n",
        "\n",
        "    def search(self, x, is_greedy=True, max_length=255):\n",
        "        # |x[0]| = (batch_size, n)\n",
        "        batch_size = x[0].size(0)\n",
        "\n",
        "        mask = self._generate_mask(x[0], x[1])\n",
        "        # |mask| = (batch_size, n)\n",
        "        x = x[0]\n",
        "\n",
        "        mask_enc = mask.unsqueeze(1).expand(mask.size(0), x.size(1), mask.size(-1))\n",
        "        mask_dec = mask.unsqueeze(1)\n",
        "        # |mask_enc| = (batch_size, n, n)\n",
        "        # |mask_dec| = (batch_size, 1, n)\n",
        "\n",
        "        z = self.emb_dropout(self._position_encoding(self.emb_enc(x)))\n",
        "        z, _ = self.encoder(z, mask_enc)\n",
        "        # |z| = (batch_size, n, hidden_size)\n",
        "\n",
        "        # Fill a vector, which has 'batch_size' dimension, with BOS value.\n",
        "        y_t_1 = x.new(batch_size, 1).zero_() + data_loader.BOS\n",
        "        # |y_t_1| = (batch_size, 1)\n",
        "        is_decoding = x.new_ones(batch_size, 1).bool()\n",
        "\n",
        "        prevs = [None for _ in range(len(self.decoder._modules) + 1)]\n",
        "        y_hats, indice = [], []\n",
        "        # Repeat a loop while sum of 'is_decoding' flag is bigger than 0,\n",
        "        # or current time-step is smaller than maximum length.\n",
        "        while is_decoding.sum() > 0 and len(indice) < max_length:\n",
        "            # Unlike training procedure,\n",
        "            # take the last time-step's output during the inference.\n",
        "            h_t = self.emb_dropout(\n",
        "                self._position_encoding(self.emb_dec(y_t_1), init_pos=len(indice))\n",
        "            )\n",
        "            # |h_t| = (batch_size, 1, hidden_size))\n",
        "            if prevs[0] is None:\n",
        "                prevs[0] = h_t\n",
        "            else:\n",
        "                prevs[0] = torch.cat([prevs[0], h_t], dim=1)\n",
        "\n",
        "            for layer_index, block in enumerate(self.decoder._modules.values()):\n",
        "                prev = prevs[layer_index]\n",
        "                # |prev| = (batch_size, len(y_hats), hidden_size)\n",
        "\n",
        "                h_t, _, _, _, _ = block(h_t, z, mask_dec, prev, None)\n",
        "                # |h_t| = (batch_size, 1, hidden_size)\n",
        "\n",
        "                if prevs[layer_index + 1] is None:\n",
        "                    prevs[layer_index + 1] = h_t\n",
        "                else:\n",
        "                    prevs[layer_index + 1] = torch.cat([prevs[layer_index + 1], h_t], dim=1)\n",
        "                # |prev| = (batch_size, len(y_hats) + 1, hidden_size)\n",
        "\n",
        "            y_hat_t = self.generator(h_t)\n",
        "            # |y_hat_t| = (batch_size, 1, output_size)\n",
        "\n",
        "            y_hats += [y_hat_t]\n",
        "            if is_greedy: # Argmax\n",
        "                y_t_1 = torch.topk(y_hat_t, 1, dim=-1)[1].squeeze(-1)\n",
        "            else: # Random sampling                \n",
        "                y_t_1 = torch.multinomial(y_hat_t.exp().view(x.size(0), -1), 1)\n",
        "            # Put PAD if the sample is done.\n",
        "            y_t_1 = y_t_1.masked_fill_(\n",
        "                ~is_decoding,\n",
        "                data_loader.PAD,\n",
        "            )\n",
        "\n",
        "            # Update is_decoding flag.\n",
        "            is_decoding = is_decoding * torch.ne(y_t_1, data_loader.EOS)\n",
        "            # |y_t_1| = (batch_size, 1)\n",
        "            # |is_decoding| = (batch_size, 1)\n",
        "            indice += [y_t_1]\n",
        "\n",
        "        y_hats = torch.cat(y_hats, dim=1)\n",
        "        indice = torch.cat(indice, dim=-1)\n",
        "        # |y_hats| = (batch_size, m, output_size)\n",
        "        # |indice| = (batch_size, m)\n",
        "\n",
        "        return y_hats, indice\n",
        "\n",
        "    #profile\n",
        "    def batch_beam_search(\n",
        "        self,\n",
        "        x,\n",
        "        beam_size=5,\n",
        "        max_length=255,\n",
        "        n_best=1,\n",
        "        length_penalty=.2,\n",
        "    ):\n",
        "        # |x[0]| = (batch_size, n)\n",
        "        batch_size = x[0].size(0)\n",
        "        n_dec_layers = len(self.decoder._modules)\n",
        "\n",
        "        mask = self._generate_mask(x[0], x[1])\n",
        "        # |mask| = (batch_size, n)\n",
        "        x = x[0]\n",
        "\n",
        "        mask_enc = mask.unsqueeze(1).expand(mask.size(0), x.size(1), mask.size(-1))\n",
        "        mask_dec = mask.unsqueeze(1)\n",
        "        # |mask_enc| = (batch_size, n, n)\n",
        "        # |mask_dec| = (batch_size, 1, n)\n",
        "\n",
        "        z = self.emb_dropout(self._position_encoding(self.emb_enc(x)))\n",
        "        z, _ = self.encoder(z, mask_enc)\n",
        "        # |z| = (batch_size, n, hidden_size)\n",
        "\n",
        "        prev_status_config = {}\n",
        "        for layer_index in range(n_dec_layers + 1):\n",
        "            prev_status_config['prev_state_%d' % layer_index] = {\n",
        "                'init_status': None,\n",
        "                'batch_dim_index': 0,\n",
        "            }\n",
        "        # Example of prev_status_config:\n",
        "        # prev_status_config = {\n",
        "        #     'prev_state_0': {\n",
        "        #         'init_status': None,\n",
        "        #         'batch_dim_index': 0,\n",
        "        #     },\n",
        "        #     'prev_state_1': {\n",
        "        #         'init_status': None,\n",
        "        #         'batch_dim_index': 0,\n",
        "        #     },\n",
        "        #\n",
        "        #     ...\n",
        "        #\n",
        "        #     'prev_state_${n_layers}': {\n",
        "        #         'init_status': None,\n",
        "        #         'batch_dim_index': 0,\n",
        "        #     }\n",
        "        # }\n",
        "\n",
        "        boards = [\n",
        "            SingleBeamSearchBoard(\n",
        "                z.device,\n",
        "                prev_status_config,\n",
        "                beam_size=beam_size,\n",
        "                max_length=max_length,\n",
        "            ) for _ in range(batch_size)\n",
        "        ]\n",
        "        done_cnt = [board.is_done() for board in boards]\n",
        "\n",
        "        length = 0\n",
        "        while sum(done_cnt) < batch_size and length <= max_length:\n",
        "            fab_input, fab_z, fab_mask = [], [], []\n",
        "            fab_prevs = [[] for _ in range(n_dec_layers + 1)]\n",
        "\n",
        "            for i, board in enumerate(boards): # i == sample_index in minibatch\n",
        "                if board.is_done() == 0:\n",
        "                    y_hat_i, prev_status = board.get_batch()\n",
        "\n",
        "                    fab_input += [y_hat_i                 ]\n",
        "                    fab_z     += [z[i].unsqueeze(0)       ] * beam_size\n",
        "                    fab_mask  += [mask_dec[i].unsqueeze(0)] * beam_size\n",
        "\n",
        "                    for layer_index in range(n_dec_layers + 1):\n",
        "                        prev_i = prev_status['prev_state_%d' % layer_index]\n",
        "                        if prev_i is not None:\n",
        "                            fab_prevs[layer_index] += [prev_i]\n",
        "                        else:\n",
        "                            fab_prevs[layer_index] = None\n",
        "\n",
        "            fab_input = torch.cat(fab_input, dim=0)\n",
        "            fab_z     = torch.cat(fab_z,     dim=0)\n",
        "            fab_mask  = torch.cat(fab_mask,  dim=0)\n",
        "            for i, fab_prev in enumerate(fab_prevs): # i == layer_index\n",
        "                if fab_prev is not None:\n",
        "                    fab_prevs[i] = torch.cat(fab_prev, dim=0)\n",
        "            # |fab_input|    = (current_batch_size, 1,)\n",
        "            # |fab_z|        = (current_batch_size, n, hidden_size)\n",
        "            # |fab_mask|     = (current_batch_size, 1, n)\n",
        "            # |fab_prevs[i]| = (current_batch_size, length, hidden_size)\n",
        "            # len(fab_prevs) = n_dec_layers + 1\n",
        "\n",
        "            # Unlike training procedure,\n",
        "            # take the last time-step's output during the inference.\n",
        "            h_t = self.emb_dropout(\n",
        "                self._position_encoding(self.emb_dec(fab_input), init_pos=length)\n",
        "            )\n",
        "            # |h_t| = (current_batch_size, 1, hidden_size)\n",
        "            if fab_prevs[0] is None:\n",
        "                fab_prevs[0] = h_t\n",
        "            else:\n",
        "                fab_prevs[0] = torch.cat([fab_prevs[0], h_t], dim=1)\n",
        "\n",
        "            for layer_index, block in enumerate(self.decoder._modules.values()):\n",
        "                prev = fab_prevs[layer_index]\n",
        "                # |prev| = (current_batch_size, m, hidden_size)\n",
        "\n",
        "                h_t, _, _, _, _ = block(h_t, fab_z, fab_mask, prev, None)\n",
        "                # |h_t| = (current_batch_size, 1, hidden_size)\n",
        "\n",
        "                if fab_prevs[layer_index + 1] is None:\n",
        "                    fab_prevs[layer_index + 1] = h_t\n",
        "                else:\n",
        "                    fab_prevs[layer_index + 1] = torch.cat(\n",
        "                        [fab_prevs[layer_index + 1], h_t],\n",
        "                        dim=1,\n",
        "                    ) # Append new hidden state for each layer.\n",
        "\n",
        "            y_hat_t = self.generator(h_t)\n",
        "            # |y_hat_t| = (batch_size, 1, output_size)\n",
        "\n",
        "            # |fab_prevs[i][begin:end]| = (beam_size, length, hidden_size)\n",
        "            cnt = 0\n",
        "            for board in boards:\n",
        "                if board.is_done() == 0:\n",
        "                    begin = cnt * beam_size\n",
        "                    end = begin + beam_size\n",
        "\n",
        "                    prev_status = {}\n",
        "                    for layer_index in range(n_dec_layers + 1):\n",
        "                        prev_status['prev_state_%d' % layer_index] = fab_prevs[layer_index][begin:end]\n",
        "\n",
        "                    board.collect_result(y_hat_t[begin:end], prev_status)\n",
        "\n",
        "                    cnt += 1\n",
        "\n",
        "            done_cnt = [board.is_done() for board in boards]\n",
        "            length += 1\n",
        "\n",
        "        batch_sentences, batch_probs = [], []\n",
        "\n",
        "        for i, board in enumerate(boards):\n",
        "            sentences, probs = board.get_n_best(n_best, length_penalty=length_penalty)\n",
        "\n",
        "            batch_sentences += [sentences]\n",
        "            batch_probs     += [probs]\n",
        "\n",
        "        return batch_sentences, batch_probs\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPZnRp_Ti72T"
      },
      "source": [
        ""
      ]
    }
  ]
}
